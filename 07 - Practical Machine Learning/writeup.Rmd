---
title: "Practical Machine Learning Project"
author: "Oszkar Jozsa"
output: html_document
---

#Introduction

This paper is a submission to Coursera's Practical Machine Learning class.

In this submission I will use a random forest method to predict whether an exercise is correct or not. 
The data set used here is the Weight Lifting Exercises Dataset of the Groupware@LES 
research group's Human Activity Recognition (HAR) dataset. 
The dataset and it's description can be found [here](http://groupware.les.inf.puc-rio.br/har).

The dataset is included with the repository so the code below can be run as is (as long the repo is in the working directory).

Also, there is a `prediciton_algorithm.R` file included with the repo. That file contains only the code (also found below), 
in a more compact form. All detail and discussion can be found in this file.

#Instructions 

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

#Data cleaning, exploratory analysis

First, let's look at the data. Also, set the seed for reproducibility.

```{r}
library(caret)
library(randomForest)
library(caret)

set.seed(6543)

data <- read.csv("pml-training.csv")
str(data$classe)
hist(as.numeric(data$classe), main="Activity type distribution", 
     xlab="Classes")
```

The source webpage tells us that Class A means properly done exercise, the four remaining ones 
indicate errors.

Looking at the data, it seems that there are few variables that mostly contain `NA`s or `#DIV/0!`s (probably meaning zero division).

```{r}
sum(complete.cases(data))
```

Only 406 observations are fully complete so instead of using the complete observations, 
we'll remove the "problematic" variables. 
We'll remove the variables that contain large amount of `NA`sor `#DIV/0!`s, say over 20% 
(if there is only a few, we might able to impute them).

```{r}
toremove <- numeric()
  
for(i in 1:length(data)) {
  # either too many NAs, too many #DIV/0s or too many empty values ("")
  if(sum(is.na(data[i])) > 0.2*nrow(data) || sum(data[i] == "#DIV/0!") > 0.2*nrow(data) || sum(data[i] == "") > 0.2*nrow(data) ) {
    # then don't include in cleaned
    toremove <- c(toremove, i)
  }
}
toremove <- c(toremove, 1:7)

cleaned <- data[, -toremove]

sum(complete.cases(cleaned))
```

The code above saved the indices of the "problematic" variables into `toremove`. Also, the first 
7 variables are included in `toremove` as they are not real measurements of activity but some descrtiptions 
of circumstances (timestamps, names, etc.)

This left us 53 variables (all numeric except of the class) which actually means that now all of our observations are complete. 
We have to be careful when it comes to testing, we should remove the same variables as we did here.

#Training and validation

I will use the random forest model; we've seen in class that it performs very well in simple cases like this.

First of all, the data should be splitted into training and cross-validation sets. 
75% will go towards the training set.

```{r}
subsamples <- createDataPartition(y=cleaned$classe, p=0.75, list=FALSE)
trainingset <- cleaned[subsamples,] 
crossvalidationset <- cleaned[-subsamples,]
```

The `randomForest` function in the randomForest package will be a great help for us here, 
since it offers a single call function for building a random forest model. 
When it's done (can take a while), let's use it for a prediction on our cross validation set 
(minus the 53rd row, the actual class we're predicting).

```{r}
model <- randomForest(classe ~. , data=trainingset, method="class")
prediction <- predict(model, crossvalidationset[,-53], type = "class")
```

Now we have our predictions using the random forest in the `prediction` variable which is a factor 
variable, just as the original `classe` variable.

Lastly, let's check the performance of this prediction, using the `confusionMatrix` function 
(compared to the gound truth, `crossvalidationset$classe`).

```{r}
confusionMatrix(prediction, crossvalidationset$classe)
```

This shows us that the random forest method indeed performs very vell in this case. 
Accuracy is 99.43% and both specificity and sensitivity are above 99% for all classes. 
We could make a heatmap of this table but that probably would't be that informative as 
99+% of the items are on the diagonal.

This accuracy suggests that most likely all 20 test cases will pass as 0.9943 * 20 = 19.886 
(in other words, the model will make one error in about 170 predictions). 
So it seems, no more tweaking is needed this time.

Most of our errors classified as the neighboring class to the correct one. 
Also, this result would be even more accurate if we would do a binary decision, merging the 4 "wrong" classes 
into a single one (thus, predicting good vs. bad technique insted of 5 classes).

#Test set

Finally, test our model with the so far unused test set.

```{r}
test_data <- read.csv("pml-testing.csv")
test_data <- test_data[,-toremove]
predict(model, test_data, type="class")
```

According to the course validation site, all 20 are correct.

#Conclusion

The random forest method was sufficient enough to build a prediction model that performs 
with almost 99.5% accuracy. All of out test cases have passed using the model we built with 
the training set.